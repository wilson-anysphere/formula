# Excel Oracle compatibility tests

This directory contains the **case corpus** and a **pinned results dataset** used to
continuously validate `crates/formula-engine` against an Excel oracle dataset.
The pinned dataset may be produced by real Excel or may be a synthetic CI baseline;
check the dataset `source.note` field for details.

If the repo is incrementally patching a synthetic baseline with **real Excel** results for small
targeted subsets (e.g. odd-coupon bond edge cases), those patches are recorded under
`source.patches` in the pinned dataset so it’s clear which Excel build produced the overwritten
case IDs.

## Quick start (one command)

From repo root:

```bash
python tools/excel-oracle/compat_gate.py
```

## Unified compatibility scorecard (corpus + Excel-oracle)

The Excel-oracle harness measures **calculation fidelity (L2)**, while the compatibility corpus measures
**read (L1)** and **round-trip preservation (L4)**.

To merge both into a single markdown table, use:

```bash
python tools/compat_scorecard.py --out-md compat_scorecard.md
# or:
python -m tools.compat_scorecard --out-md compat_scorecard.md
```

By default it looks for:

- `tools/corpus/out/**/summary.json` (generated by `python -m tools.corpus.dashboard`)
- `tests/compatibility/excel-oracle/reports/mismatch-report.json` (generated by this harness)

CI also ships a workflow-run aggregator (`.github/workflows/compat-scorecard.yml`) that downloads the two
artifacts and uploads a unified `compat-scorecard` report.

Tip: the aggregator workflow supports manual dispatch (`workflow_dispatch`) with an optional SHA input for
backfills/debugging.

To see what the gate would run (resolved dataset paths + commands) without executing `cargo`, use:

```bash
python tools/excel-oracle/compat_gate.py --dry-run
```

This runs the default **smoke** tier gate:

1. `cargo run -p formula-excel-oracle` → writes `datasets/engine-results.json`
2. `python tools/excel-oracle/compare.py` → writes `reports/mismatch-report.json`

It also writes a human-readable summary to:

* `reports/summary.md`

and exits non-zero if mismatches exceed the configured threshold.

## Repository layout

- `cases.json` — curated (~2k) formula + input-grid cases (deterministic).
- Supplemental small corpora for targeted Windows + Excel runs:
  - `tools/excel-oracle/odd_coupon_long_stub_cases.json` — long odd-coupon (`ODDF*` / `ODDL*`) **long-stub**
    scenarios (DFC/E > 1 or DSM/E > 1). This subset reuses the canonical `caseId`s from `cases.json`.
  - `tools/excel-oracle/odd_coupon_boundary_cases.json` — boundary date-equality scenarios
    (e.g. `issue == settlement`, `settlement == first_coupon`). This subset reuses the canonical `caseId`s from
    `cases.json` so results can be merged back into the pinned dataset.
  - `tools/excel-oracle/odd_coupon_invalid_schedule_cases.json` — odd-coupon **invalid schedule** scenarios
    (cases covering schedule alignment / misalignment; some return `#NUM!`, while others are accepted by the
    engine today and should be validated against real Excel). This subset reuses the canonical `caseId`s from
    `cases.json` so results can be merged back into the pinned dataset.
  - `tools/excel-oracle/odd_coupon_basis4_cases.json` — odd-coupon **basis=4** (European 30/360) scenarios.
    This subset reuses the canonical `caseId`s from `cases.json` so results can be merged back into the pinned dataset.
  - `tools/excel-oracle/odd_coupon_validation_cases.json` — validation scenarios for **negative yields / negative coupon rates**
    and yield-domain boundaries (tagged `odd_coupon_validation` in the canonical corpus). This subset reuses the canonical
    `caseId`s from `cases.json` so results can be merged back into the pinned dataset.

  These subset corpora are derived from the canonical `cases.json` by tag filters. To regenerate them after editing the
  canonical corpus, run:

  ```bash
  python tools/excel-oracle/regenerate_subset_corpora.py
  ```

  To only verify the subset corpora are up to date (without rewriting files), run:

  ```bash
  python tools/excel-oracle/regenerate_subset_corpora.py --check
  ```

  To preview what would be written (case counts + paths) without writing files, run:

  ```bash
  python tools/excel-oracle/regenerate_subset_corpora.py --dry-run
  ```
- `datasets/` — results datasets:
  - `excel-oracle.pinned.json` — pinned Excel dataset for CI (no Excel needed).
  - `versioned/` — version-tagged pinned datasets (useful when Excel behavior differs across versions/builds).
    Versioned datasets include the `cases.json` SHA-256 prefix in the filename (`*-cases-<sha8>.json`) so
    `compat_gate.py` can auto-select the right dataset.
  - `engine-results.json` — generated locally/CI by the engine runner.
- `reports/` — mismatch reports produced by `compare.py`.

Note: the repo pins a dataset covering the full corpus so CI can validate
engine behavior even when Excel is not available. CI runtime is controlled via
tag filtering (see `compat_gate.py --tier smoke`), not by shrinking the pinned
dataset.

For the true Excel oracle, regenerate the dataset with the Windows + Excel
runner and pin it (see below).

In early development the repo may also pin a **synthetic baseline** dataset
(generated by `crates/formula-excel-oracle`) so the harness and CI wiring stay
alive before we have a full Excel-generated oracle. Check the dataset's
`source.note` field for details.

The case corpus generator (`tools/excel-oracle/generate_cases.py`) validates that:

- every `non_volatile` function in `shared/functionCatalog.json` has at least one **case formula** (`case.formula`)
- `volatile` catalog functions are excluded (keeps pinned oracle comparisons deterministic)

The Rust test suite also enforces these invariants (see
`crates/formula-engine/tests/excel_oracle_coverage.rs` and
`crates/formula-engine/tests/function_catalog_sync.rs`) so drift is caught even if
`cases.json` is edited without re-running the generator.

## Tags and filtering

Each case has `tags` that can be used to include/exclude subsets when evaluating
or comparing.

This lets CI stay fast (start with a small tag set) while still enabling full
corpus runs locally or on a self-hosted Windows runner.

`compat_gate.py` defaults to a curated tag set (basic arithmetic/comparison + a
few baseline functions), plus a small amount of **dynamic array spill coverage**
(`range`, `TRANSPOSE`, `SEQUENCE`).

The default tag slice also includes representative **value coercion / conversion**
coverage (tagged `coercion` / `VALUE` / `DATEVALUE` / `TIMEVALUE`), so changes to
text→number/date/time semantics are exercised in CI even before a full Excel
oracle dataset is generated.

### Excel 365+ LET/LAMBDA + higher-order functions

The corpus also includes a small, deterministic set of cases tagged `lambda`
covering:

* `LET` (basic binding, multi-binding, shadowing)
* `LAMBDA` invocation patterns
* Higher-order array functions: `MAP`, `REDUCE`, `SCAN`, `BYROW`, `BYCOL`, `MAKEARRAY`

These cases are **not included** in the `compat_gate.py --tier smoke` preset until
they are validated against a pinned dataset produced by **Windows + Microsoft Excel**
(see below).

## Tiered compatibility gates (`--tier`)

`compat_gate.py` supports tier presets so you can run progressively larger
Excel-compatibility checks without hand-editing `--include-tag` lists.

The authoritative preset definitions live in `tools/excel-oracle/compat_gate.py`
as `SMOKE_INCLUDE_TAGS` / `P0_INCLUDE_TAGS`.

Note: include-tag filtering uses **OR semantics** (a case is included if it has
*any* of the include tags).

### Smoke (default, CI-friendly)

Runs a small, high-signal slice of the corpus (fast).

Preset include tags:

`add`, `sub`, `mul`, `div`, `cmp`, `SUM`, `IF`, `IFERROR`, `error`, `range`, `TRANSPOSE`, `SEQUENCE`, `COUNT`, `COUNTIF`, `TEXT`, `TEXTSPLIT`, `VALUE`, `DATEVALUE`, `WORKDAY`, `NETWORKDAYS`, `XLOOKUP`, `XMATCH`, `FILTER`, `SORT`, `UNIQUE`, `ISERROR`, `thai`, `boundary`, `odd_coupon`, `coupon_schedule`, `coercion`

```bash
python tools/excel-oracle/compat_gate.py --tier smoke
# or just:
python tools/excel-oracle/compat_gate.py
```

### P0 (broader common-function slice)

Runs a broader but still bounded set that aims to cover the most common
functions/operators in the curated corpus.

Preset include tags:

`arith`, `cmp`, `math`, `agg`, `logical`, `text`, `date`, `lookup`, `spill`, `dynarr`, `error`, `info`, `coercion`, `thai`

```bash
python tools/excel-oracle/compat_gate.py --tier p0
```

### Full (no include-tag filtering)

Runs the full case corpus (no `--include-tag` filtering unless you explicitly
pass `--include-tag`).

```bash
python tools/excel-oracle/compat_gate.py --tier full
```

### Precedence / interaction with tag flags

- If you pass **any** `--include-tag`, it **overrides** the tier preset.
- `--exclude-tag` always applies (regardless of tier).

### Runtime tradeoffs & pinned datasets

- `--tier smoke` is intended to stay fast for CI.
- `--tier p0` and `--tier full` are better suited for local runs or nightly CI.

The gate **does not require Excel** at runtime; it compares against a pinned
Excel dataset (`datasets/excel-oracle.pinned.json` or the newest matching file in
`datasets/versioned/` for the current `cases.json` hash).

Important: the pinned dataset must contain results for the cases you choose to
run. Using tag filtering (tiers or explicit `--include-tag`) is recommended to
control runtime:

```bash
python tools/excel-oracle/compat_gate.py --tier full --max-cases 50
```

`compare.py` also verifies that the expected/actual datasets embed a
`caseSet.sha256` matching the current `cases.json`. If you regenerate or edit
the corpus, you must also regenerate/pin a matching expected dataset (or add a
new versioned dataset for the new corpus hash).

## Regenerate the case corpus

From repo root:

```bash
python tools/excel-oracle/generate_cases.py --out tests/compatibility/excel-oracle/cases.json
```

## Generate Excel oracle results (Windows + Excel required)

On a Windows machine with Microsoft Excel installed:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

Tip: pass `-DryRun` to preview how many cases would be run (after tag filtering / MaxCases) without launching Excel.

To generate only a subset of cases (by tag):

- `-IncludeTags` uses **OR semantics** (a case is included if it matches *any* of the tags)
- `-RequireTags` uses **AND semantics** (a case is included only if it matches *all* tags)

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json `
  -IncludeTags SUM,IF,cmp `
  -RequireTags odd_coupon,basis4
```

Then pin the dataset for CI (and optionally write a version-tagged copy):

```bash
python tools/excel-oracle/pin_dataset.py \
  --dataset tests/compatibility/excel-oracle/datasets/excel-oracle.json \
  --pinned tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --versioned-dir tests/compatibility/excel-oracle/datasets/versioned
```

### Patch the pinned dataset with real Excel results (subset, merge-friendly)

If you only need real Excel results for a targeted slice (e.g. odd-coupon bonds), prefer patching
the existing pinned dataset instead of regenerating the full corpus. This overwrites only the
matching `caseId`s and keeps diffs smaller / merges friendlier:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_boundary_cases.json
```

Other useful subset corpora include:

- `tools/excel-oracle/odd_coupon_long_stub_cases.json`
- `tools/excel-oracle/odd_coupon_basis4_cases.json`
- `tools/excel-oracle/odd_coupon_validation_cases.json`
- `tools/excel-oracle/odd_coupon_invalid_schedule_cases.json`

See `tools/excel-oracle/README.md` for more examples.

To validate the dataset and preview the output paths without writing files, use:

```bash
python tools/excel-oracle/pin_dataset.py --dataset /path/to/dataset.json --dry-run
```

## Incremental pinned dataset updates (smaller diffs)

If you *only added new cases* to `cases.json` (i.e. existing `caseId`s remain valid), regenerating the
entire pinned dataset can produce a very large diff.

You can instead fill in only the missing case results:

```bash
python tools/excel-oracle/update_pinned_dataset.py
```

To preview what would change (and whether the updater would run the engine) without writing files, use:

```bash
python tools/excel-oracle/update_pinned_dataset.py --dry-run
```

Note: `compat_gate.py` prefers the matching dataset under `datasets/versioned/` (by `cases.json` hash)
when present. `update_pinned_dataset.py` updates `excel-oracle.pinned.json` **and** refreshes the
matching versioned copy by default; pass `--no-versioned` to disable.

## Run the engine runner directly

```bash
cargo run -p formula-excel-oracle -- \
  --cases tests/compatibility/excel-oracle/cases.json \
  --out   tests/compatibility/excel-oracle/datasets/engine-results.json \
  --include-tag add --include-tag sub --include-tag mul --include-tag div
```

## Run comparison directly

```bash
python tools/excel-oracle/compare.py \
  --cases    tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual   tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report   tests/compatibility/excel-oracle/reports/mismatch-report.json
```

To preview how many cases would be compared (after tag filtering / `--max-cases`) without writing a report file, use:

```bash
python tools/excel-oracle/compare.py --dry-run \
  --cases    tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual   tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report   tests/compatibility/excel-oracle/reports/mismatch-report.json
```

### Numeric tolerances

`compare.py` uses a tight default numeric tolerance (`abs=rel=1e-9`). Some functions are
inherently iterative (for example yield solvers), and may differ from Excel by small floating-point
amounts even when the math is correct.

You can override tolerances for a tag (or set of tags) without loosening the whole corpus:

```bash
python tools/excel-oracle/compare.py \
  --cases    tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual   tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report   tests/compatibility/excel-oracle/reports/mismatch-report.json \
  --tag-abs-tol odd_coupon=1e-6 \
  --tag-rel-tol odd_coupon=1e-6
```
