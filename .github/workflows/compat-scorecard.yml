name: Compatibility Scorecard

on:
  workflow_run:
    workflows:
      - corpus
      - Excel Compatibility (Oracle)
    types: [completed]
  workflow_dispatch:
    inputs:
      sha:
        description: "Optional commit SHA to generate a scorecard for (defaults to current ref SHA)."
        required: false
        default: ""
        type: string

concurrency:
  group: compat-scorecard-${{ github.event.workflow_run.head_sha || github.event.inputs.sha || github.sha }}
  cancel-in-progress: true

permissions:
  contents: read
  actions: read

jobs:
  compat-scorecard:
    name: Unified compatibility scorecard
    # Pin runner image versions for reproducibility. GitHub's `ubuntu-latest` alias
    # moves over time, which can unexpectedly break automation scripts.
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v4

      - uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548 # v5
        with:
          python-version: "3.11"

      - name: Find related workflow runs (by head_sha)
        id: runs
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v7
        env:
          TARGET_SHA: ${{ github.event.inputs.sha }}
        with:
          script: |
            const manualSha = (process.env.TARGET_SHA || "").trim();
            const sha = manualSha || (context.payload.workflow_run ? context.payload.workflow_run.head_sha : context.sha);
            const triggeredName = context.payload.workflow_run ? context.payload.workflow_run.name : "manual";
            const owner = context.repo.owner;
            const repo = context.repo.repo;

            async function findLatestRun(workflowFile) {
              // Scan recent runs for this workflow until we find a matching head SHA.
              //
              // For workflow_run triggers the matching run is almost always in the first page, but
              // for manual backfills (workflow_dispatch with an older sha) we may need to paginate.
              const perPage = 100;
              const maxPages = 10; // cap to avoid excessive API calls
              for (let page = 1; page <= maxPages; page++) {
                const resp = await github.rest.actions.listWorkflowRunsForWorkflow({
                  owner,
                  repo,
                  workflow_id: workflowFile,
                  per_page: perPage,
                  page,
                });
                const runs = resp.data.workflow_runs || [];
                for (const run of runs) {
                  if (run.head_sha !== sha) continue;
                  return run;
                }
                if (runs.length < perPage) {
                  break;
                }
              }
              return null;
            }

            let corpusRunId = null;
            let excelRunId = null;
            let shouldGenerate = false;

            if (triggeredName === "corpus") {
              corpusRunId = context.payload.workflow_run.id;
              const excel = await findLatestRun("excel-compat.yml");
              if (excel && excel.status !== "completed") {
                // Excel workflow is in progress; wait for it to complete so we don't emit a partial
                // scorecard and then immediately re-run.
                shouldGenerate = false;
              } else {
                excelRunId = excel ? excel.id : null;
                // If Excel didn't run for this commit, there is no calc metric to report. Avoid
                // emitting a corpus-only scorecard.
                shouldGenerate = !!excelRunId;
              }
            } else if (triggeredName === "Excel Compatibility (Oracle)") {
              excelRunId = context.payload.workflow_run.id;
              const corpus = await findLatestRun("corpus.yml");
              if (corpus && corpus.status !== "completed") {
                // Corpus workflow is in progress; wait so we can generate a full scorecard in one
                // shot. If corpus never ran, `corpus.any` will be null and we'll generate a partial
                // scorecard (missing corpus metrics).
                shouldGenerate = false;
              } else {
                corpusRunId = corpus ? corpus.id : null;
                // Always generate after the Excel workflow; corpus metrics are optional and will be
                // marked missing when the corpus workflow did not run for this commit.
                shouldGenerate = true;
              }
            } else {
              // Unexpected; fall back to searching both.
              const corpus = await findLatestRun("corpus.yml");
              const excel = await findLatestRun("excel-compat.yml");
              const anyPending = (corpus && corpus.status !== "completed") || (excel && excel.status !== "completed");
              if (anyPending) {
                shouldGenerate = false;
              } else {
                corpusRunId = corpus ? corpus.id : null;
                excelRunId = excel ? excel.id : null;
                shouldGenerate = !!excelRunId;
              }
            }

            core.setOutput("sha", sha);
            core.setOutput("corpus_run_id", corpusRunId || "");
            core.setOutput("excel_run_id", excelRunId || "");
            core.setOutput("generate", shouldGenerate ? "true" : "false");

            if (!shouldGenerate) {
              core.notice(
                `Not generating scorecard yet for sha ${sha}: corpus=${corpusRunId} excel=${excelRunId}`
              );
            }

      - name: Download corpus summary artifact
        if: ${{ steps.runs.outputs.generate == 'true' && steps.runs.outputs.corpus_run_id != '' }}
        continue-on-error: true
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v4
        with:
          name: corpus-public-summary
          path: artifacts/corpus
          run-id: ${{ steps.runs.outputs.corpus_run_id }}
          github-token: ${{ github.token }}

      - name: Download corpus private summary artifact
        if: ${{ steps.runs.outputs.generate == 'true' && steps.runs.outputs.corpus_run_id != '' }}
        continue-on-error: true
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v4
        with:
          name: corpus-private-summary
          path: artifacts/corpus
          run-id: ${{ steps.runs.outputs.corpus_run_id }}
          github-token: ${{ github.token }}

      - name: Download excel-oracle artifacts
        if: ${{ steps.runs.outputs.generate == 'true' && steps.runs.outputs.excel_run_id != '' }}
        continue-on-error: true
        id: download_excel_summary
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v4
        with:
          name: excel-oracle-summary
          path: artifacts/excel
          run-id: ${{ steps.runs.outputs.excel_run_id }}
          github-token: ${{ github.token }}

      - name: Download excel-oracle artifacts (fallback)
        if: ${{ steps.runs.outputs.generate == 'true' && steps.runs.outputs.excel_run_id != '' && steps.download_excel_summary.outcome == 'failure' }}
        continue-on-error: true
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v4
        with:
          name: excel-oracle-artifacts
          path: artifacts/excel
          run-id: ${{ steps.runs.outputs.excel_run_id }}
          github-token: ${{ github.token }}

      - name: Generate unified scorecard
        if: ${{ steps.runs.outputs.generate == 'true' }}
        env:
          # Prefer rendering the commit we are summarizing, not the workflow_run runner checkout SHA.
          GITHUB_SHA: ${{ steps.runs.outputs.sha }}
          # Point the scorecard's "Run:" metadata at the Excel-compat run, since that's where the
          # calc fidelity signal comes from. Corpus run URLs are included in the Inputs section
          # when present.
          GITHUB_RUN_ID: ${{ steps.runs.outputs.excel_run_id }}
        run: |
          CORPUS_SUMMARY="artifacts/corpus/tools/corpus/out/public/summary.json"
          PRIVACY_FLAGS=()
          if [ -f artifacts/corpus/tools/corpus/out/private/summary.json ]; then
            CORPUS_SUMMARY="artifacts/corpus/tools/corpus/out/private/summary.json"
            PRIVACY_FLAGS+=(--privacy-mode private)
          fi

          python tools/compat_scorecard.py \
            --corpus-summary "$CORPUS_SUMMARY" \
            --oracle-report artifacts/excel/tests/compatibility/excel-oracle/reports/mismatch-report.json \
            --out-md artifacts/compat_scorecard.md \
            --out-json artifacts/compat_scorecard.json \
            --allow-missing-inputs \
            "${PRIVACY_FLAGS[@]}"

          if [ -f artifacts/compat_scorecard.md ]; then
            cat artifacts/compat_scorecard.md >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload unified scorecard
        if: ${{ steps.runs.outputs.generate == 'true' }}
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v4
        with:
          name: compat-scorecard
          path: |
            artifacts/compat_scorecard.md
            artifacts/compat_scorecard.json
          if-no-files-found: ignore
