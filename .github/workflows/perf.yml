name: Performance

on:
  push:
    branches: [main]
  pull_request:
    paths:
      - "apps/desktop/**"
      - "crates/**"
      - "packages/**"
      - "scripts/**"
      - "Cargo.toml"
      - "Cargo.lock"
      - "pnpm-lock.yaml"
      - "package.json"
      - "rust-toolchain.toml"
      - ".github/workflows/perf.yml"

env:
  # Keep Node pinned to the same major as CI/release workflows to reduce drift in
  # scripts and build tooling.
  NODE_VERSION: 22
  # Keep in sync with release.yml/ci.yml so performance runs don't break when
  # wasm-pack publishes a new incompatible version.
  WASM_PACK_VERSION: 0.13.1
  # Opt-in: bundle Pyodide assets into `apps/desktop/dist` (otherwise Pyodide is downloaded
  # on-demand at runtime and cached in the app data directory).
  FORMULA_BUNDLE_PYODIDE_ASSETS: ${{ vars.FORMULA_BUNDLE_PYODIDE_ASSETS }}

jobs:
  conflict-marker-guard:
    name: "Guard: no merge conflict markers"
    # Pin runner image versions for reproducibility. GitHub's `ubuntu-latest` alias
    # moves over time, which can introduce unexpected breakages in guard checks.
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v4
      - name: Fail if merge conflict markers are present
        run: bash scripts/ci/check-merge-conflict-markers.sh
      - name: "Guard: Rust toolchain pins match rust-toolchain.toml"
        run: bash scripts/ci/check-rust-toolchain-pins.sh

  benchmark:
    needs: conflict-marker-guard
    # Desktop startup benchmarks require WebKitGTK 4.1 (Ubuntu 24.04+) and a headless display via Xvfb.
    runs-on: ubuntu-24.04
    timeout-minutes: 45
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v4

      - uses: pnpm/action-setup@41ff72655975bd51cab0327fa583b6e92b6d3061 # v4
        with:
          # Pin pnpm patch version for deterministic builds (keep in sync with package.json).
          version: 9.0.0

      - uses: actions/setup-node@6044e13b5dc448c55e2357c09f80417699197238 # v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: pnpm
          cache-dependency-path: pnpm-lock.yaml

      - uses: dtolnay/rust-toolchain@e97e2d8cc328f1b50210efc529dca0028893a2d9 # v1
        with:
          toolchain: 1.92.0
      - uses: Swatinem/rust-cache@779680da715d629ac1d338a641029a2f4372abb5 # v2

      # Our dev scripts default to a repo-local CARGO_HOME to avoid cross-agent
      # contention on shared ~/.cargo. In GitHub Actions we prefer the default
      # CARGO_HOME so the shared cache action works as intended.
      - name: Use shared Cargo home for CI caching
        run: echo "CARGO_HOME=$HOME/.cargo" >> "$GITHUB_ENV"

      - name: Install JS dependencies
        run: pnpm install --frozen-lockfile

      - name: Decide whether to run real desktop startup benchmarks
        id: desktop_bench
        shell: bash
        run: |
          set -euo pipefail

          # Always run on push-to-main (the performance gate).
          if [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "run=true" >> "$GITHUB_OUTPUT"
            echo "runs=10" >> "$GITHUB_OUTPUT"
            # Full app benchmark (requires built frontend assets).
            echo "kind=full" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Run on same-repo PRs. Skip fork PRs for security/toolchain stability.
          if [[ "${{ github.event_name }}" == "pull_request" && "${{ github.event.pull_request.head.repo.full_name }}" == "${{ github.repository }}" ]]; then
            echo "run=true" >> "$GITHUB_OUTPUT"
            echo "runs=5" >> "$GITHUB_OUTPUT"
            # Full end-to-end startup benchmark (requires built frontend assets).
            echo "kind=full" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "run=false" >> "$GITHUB_OUTPUT"
          echo "runs=5" >> "$GITHUB_OUTPUT"
          echo "kind=shell" >> "$GITHUB_OUTPUT"

      - name: Install Linux dependencies (Tauri/WebView + Xvfb)
        if: steps.desktop_bench.outputs.run == 'true'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgtk-3-dev \
            libwebkit2gtk-4.1-dev \
            libayatana-appindicator3-dev \
            librsvg2-dev \
            libssl-dev \
            patchelf \
            xvfb

      - name: Install wasm32 target (required for desktop renderer build)
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        run: rustup target add wasm32-unknown-unknown

      - name: Cache wasm-pack binary
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        id: wasm-pack-cache
        uses: actions/cache@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v4
        with:
          path: ~/.cargo/bin/wasm-pack
          key: wasm-pack-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('rust-toolchain.toml') }}-v${{ env.WASM_PACK_VERSION }}
          restore-keys: |
            wasm-pack-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('rust-toolchain.toml') }}-

      - name: Cache wasm-pack tool downloads (Linux)
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        uses: actions/cache@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v4
        with:
          # wasm-pack downloads wasm-bindgen + binaryen (wasm-opt) into this cache dir.
          path: ~/.cache/.wasm-pack
          key: wasm-pack-tools-${{ runner.os }}-${{ runner.arch }}-v${{ env.WASM_PACK_VERSION }}
          restore-keys: |
            wasm-pack-tools-${{ runner.os }}-${{ runner.arch }}-

      - name: Install wasm-pack (required for @formula/engine WASM build)
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full' && steps.wasm-pack-cache.outputs.cache-hit != 'true'
        # Pinned to match the tagged release workflow so perf runs don't break
        # when wasm-pack publishes a new incompatible version.
        # Use --force so cache restore-keys (or other CI caches) can't strand a stale/untracked
        # wasm-pack binary that would otherwise block `cargo install` from overwriting it.
        run: cargo install wasm-pack --version ${{ env.WASM_PACK_VERSION }} --locked --force

      - name: Verify wasm-pack version
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        shell: bash
        run: |
          set -euo pipefail
          expected="${WASM_PACK_VERSION}"
          actual="$(wasm-pack --version | tr -d '\r' | awk '{print $2}')"
          if [[ "${actual}" != "${expected}" ]]; then
            echo "Expected wasm-pack ${expected}, but found ${actual}." >&2
            exit 1
          fi

      - name: Detect Pyodide version (for caching)
        if: env.FORMULA_BUNDLE_PYODIDE_ASSETS == '1' && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        id: pyodide
        shell: bash
        run: |
          set -euo pipefail
          version="$(node -e "const fs=require('node:fs'); const src=fs.readFileSync('apps/desktop/scripts/ensure-pyodide-assets.mjs','utf8'); const m=src.match(/const\\s+PYODIDE_VERSION\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]/); if(!m) throw new Error('PYODIDE_VERSION not found'); process.stdout.write(m[1]);")"
          echo "version=${version}" >> "$GITHUB_OUTPUT"

      - name: Restore Pyodide asset cache
        if: env.FORMULA_BUNDLE_PYODIDE_ASSETS == '1' && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        id: pyodide-cache
        uses: actions/cache/restore@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v4
        with:
          path: apps/desktop/public/pyodide/v${{ steps.pyodide.outputs.version }}/full/
          key: pyodide-${{ runner.os }}-${{ steps.pyodide.outputs.version }}-${{ hashFiles('apps/desktop/scripts/ensure-pyodide-assets.mjs') }}

      - name: Ensure Pyodide assets are present
        if: env.FORMULA_BUNDLE_PYODIDE_ASSETS == '1' && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        run: node apps/desktop/scripts/ensure-pyodide-assets.mjs

      - name: Save Pyodide asset cache
        if: env.FORMULA_BUNDLE_PYODIDE_ASSETS == '1' && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full' && steps.pyodide-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v4
        with:
          path: apps/desktop/public/pyodide/v${{ steps.pyodide.outputs.version }}/full/
          key: pyodide-${{ runner.os }}-${{ steps.pyodide.outputs.version }}-${{ hashFiles('apps/desktop/scripts/ensure-pyodide-assets.mjs') }}

      - name: Build desktop frontend assets (Vite â†’ tauri.conf.json build.frontendDist)
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        run: pnpm build:desktop

      - name: Desktop dist asset report
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        run: node scripts/desktop_dist_asset_report.mjs --json-out desktop-dist-assets-report.json
        env:
          # Optional: set as GitHub Actions variables to enable gating.
          FORMULA_DESKTOP_DIST_TOTAL_BUDGET_MB: ${{ vars.FORMULA_DESKTOP_DIST_TOTAL_BUDGET_MB }}
          FORMULA_DESKTOP_DIST_SINGLE_FILE_BUDGET_MB: ${{ vars.FORMULA_DESKTOP_DIST_SINGLE_FILE_BUDGET_MB }}

      - name: Upload desktop dist asset report (JSON)
        if: always() && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v4
        with:
          name: desktop-dist-assets-report
          path: desktop-dist-assets-report.json
          if-no-files-found: ignore

      - name: Build desktop binary (release, desktop feature)
        id: build_desktop_binary
        if: steps.desktop_bench.outputs.run == 'true'
        env:
          # `scripts/cargo_agent.sh` defaults `CARGO_PROFILE_RELEASE_CODEGEN_UNITS` based on its job count
          # for stability on multi-agent hosts. Override so the perf run matches the repo's Cargo.toml
          # release profile (codegen-units = 1), keeping both runtime perf and binary-size reporting
          # aligned with shipped release artifacts.
          CARGO_PROFILE_RELEASE_CODEGEN_UNITS: "1"
        run: |
          bash scripts/cargo_agent.sh build \
            -p formula-desktop-tauri \
            --features desktop \
            --bin formula-desktop \
            --release \
            --locked

      - name: Cache cargo-bloat binary
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        id: cargo-bloat-cache
        uses: actions/cache@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v4
        with:
          path: ~/.cargo/bin/cargo-bloat
          key: cargo-bloat-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('rust-toolchain.toml') }}-locked
          restore-keys: |
            cargo-bloat-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('rust-toolchain.toml') }}-

      - name: Install cargo-bloat
        if: steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full' && steps.cargo-bloat-cache.outputs.cache-hit != 'true'
        continue-on-error: true
        run: cargo install cargo-bloat --locked

      - name: Run benchmark suite
        id: perf
        env:
          # Enable the real desktop startup benchmark (Tauri shell) only when we've built it.
          FORMULA_RUN_DESKTOP_STARTUP_BENCH: ${{ steps.desktop_bench.outputs.run == 'true' && steps.build_desktop_binary.outcome == 'success' && '1' || '0' }}
          # Keep the metric names stable for CI gating by explicitly using cold-start mode
          # (which also produces the legacy `desktop.startup.*` aliases).
          FORMULA_DESKTOP_STARTUP_MODE: cold
          # Optional: full desktop idle-memory benchmark (Linux RSS, process tree) after TTI.
          # This is gated because it requires a built binary + the built frontend assets.
          FORMULA_RUN_DESKTOP_MEMORY_BENCH: ${{ steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full' && steps.build_desktop_binary.outcome == 'success' && '1' || '0' }}
          FORMULA_DESKTOP_IDLE_RSS_TARGET_MB: ${{ vars.FORMULA_DESKTOP_IDLE_RSS_TARGET_MB || '100' }}
          FORMULA_DESKTOP_MEMORY_RUNS: ${{ steps.desktop_bench.outputs.runs }}
          FORMULA_DESKTOP_MEMORY_TIMEOUT_MS: 20000
          # Give the app a short window to settle after TTI before sampling RSS.
          FORMULA_DESKTOP_MEMORY_SETTLE_MS: 3000
          # Desktop startup benchmark kind: `shell` is lightweight (no apps/desktop/dist required),
          # while `full` measures end-to-end app bootstrap.
          FORMULA_DESKTOP_STARTUP_BENCH_KIND: ${{ steps.desktop_bench.outputs.kind }}
          # Keep PR runtime reasonable; use more iterations on main for stable signals.
          FORMULA_DESKTOP_STARTUP_RUNS: ${{ steps.desktop_bench.outputs.runs }}
          FORMULA_DESKTOP_STARTUP_TIMEOUT_MS: 20000
          # CI only needs to gate on `window_visible_ms` + `tti_ms` (500ms / 1000ms targets).
          # Keep other desktop startup metrics permissive to avoid spurious failures on noisy runners.
          FORMULA_DESKTOP_COLD_FIRST_RENDER_TARGET_MS: ${{ vars.FORMULA_DESKTOP_COLD_FIRST_RENDER_TARGET_MS || '2000' }}
          FORMULA_DESKTOP_WEBVIEW_LOADED_TARGET_MS: ${{ vars.FORMULA_DESKTOP_WEBVIEW_LOADED_TARGET_MS || '2000' }}
          FORMULA_DESKTOP_RSS_TARGET_MB: ${{ vars.FORMULA_DESKTOP_RSS_TARGET_MB || '500' }}
          # Ensure the benchmark runs headless via scripts/xvfb-run-safe.sh.
          # (desktopStartupBench.ts uses Xvfb only when DISPLAY is unset/empty.)
          DISPLAY: ""
        run: pnpm benchmark
        continue-on-error: true

      - name: Verify desktop startup metrics are present
        # If we decided to run the real desktop (Tauri) benchmarks, enforce that the expected
        # metrics were actually produced (so we don't silently skip the perf gate due to a config
        # regression).
        if: always() && steps.desktop_bench.outputs.run == 'true' && steps.build_desktop_binary.outcome == 'success'
        env:
          DESKTOP_STARTUP_BENCH_KIND: ${{ steps.desktop_bench.outputs.kind }}
          RUN_DESKTOP_MEMORY_BENCH: ${{ steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full' && steps.build_desktop_binary.outcome == 'success' && '1' || '0' }}
        run: |
          node - <<'NODE'
          const fs = require('node:fs');
          const path = 'benchmark-results.json';
          if (!fs.existsSync(path)) {
            console.error(`[perf] ${path} not found; desktop startup metrics could not be verified.`);
            process.exit(1);
          }
          const results = JSON.parse(fs.readFileSync(path, 'utf8'));
           if (!Array.isArray(results)) {
             console.error(`[perf] ${path} is not a JSON array; got ${typeof results}`);
             process.exit(1);
           }
           const names = new Set(results.map((r) => r && r.name).filter((n) => typeof n === 'string'));
           const kind = String(process.env.DESKTOP_STARTUP_BENCH_KIND || '').trim().toLowerCase();
           const runMemory = String(process.env.RUN_DESKTOP_MEMORY_BENCH || '').trim() === '1';
           const prefix = kind === 'shell' ? 'desktop.shell_startup' : 'desktop.startup';
           const required = [
             // Explicit cold-start metrics (preferred for CI perf gating).
             `${prefix}.cold.window_visible_ms.p95`,
             `${prefix}.cold.tti_ms.p95`,
             // Legacy aliases (kept for backwards compatibility + stable historical tracking).
             `${prefix}.window_visible_ms.p95`,
             `${prefix}.tti_ms.p95`,
           ];
           if (kind !== 'shell') {
             // First render is only recorded for the full app startup benchmark.
             required.push(`${prefix}.cold.first_render_ms.p95`, `${prefix}.first_render_ms.p95`);
           }
           if (runMemory) {
             required.push('desktop.memory.idle_rss_mb.p95');
           }
            const missing = required.filter((n) => !names.has(n));
            if (missing.length) {
              console.error(`[perf] Missing desktop startup benchmark metrics: ${missing.join(', ')}`);
              console.error(
               `[perf] Startup bench kind=${JSON.stringify(kind || 'full')} (expected metrics: ${required.join(', ')})`,
            );
            console.error('[perf] Ensure FORMULA_RUN_DESKTOP_STARTUP_BENCH=1 and the desktop binary emits [startup] metrics.');
            process.exit(1);
          }
          console.log(`[perf] Desktop startup metrics present: ${required.join(', ')}`);
          NODE
      
      - name: Run TabCompletionEngine benchmark
        if: always()
        run: node packages/ai-completion/bench/tabCompletionEngine.bench.mjs --output benchmark-results.tab-completion.json --details benchmark-details.tab-completion.json

      - name: Merge TabCompletionEngine results into benchmark-results.json
        if: always()
        run: node scripts/merge-benchmark-results.mjs benchmark-results.json benchmark-results.tab-completion.json
      
      - name: Upload merged benchmark results (JSON)
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v4
        with:
          name: perf-benchmark-results
          path: benchmark-results.json
          if-no-files-found: ignore

      - name: Publish benchmark results (PR comment)
        # Fork PRs get a read-only token; skip publishing/commenting there.
        # Main-branch pushes publish in a separate job with a shared concurrency group
        # to avoid gh-pages push races with other benchmark workflows.
        if: |
          always() &&
          github.event_name == 'pull_request' &&
          github.event.pull_request.head.repo.full_name == github.repository
        uses: benchmark-action/github-action-benchmark@4bdcce38c94cec68da58d012ac24b7b1155efe8b # v1
        with:
          tool: customSmallerIsBetter
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          alert-threshold: '120%'
          comment-on-alert: true
          fail-on-alert: true

      - name: Report desktop binary size breakdown (cargo-bloat)
        if: always() && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full' && steps.build_desktop_binary.outcome == 'success'
        # Informational by default. To turn this into a gate, set the repo variable
        # `FORMULA_ENFORCE_DESKTOP_BINARY_SIZE=1` (and a `FORMULA_DESKTOP_BINARY_SIZE_LIMIT_MB` budget).
        continue-on-error: ${{ !contains(fromJson('["1","true","True","TRUE","yes","Yes","YES","y","Y","on","On","ON"]'), vars.FORMULA_ENFORCE_DESKTOP_BINARY_SIZE) }}
        env:
          FORMULA_ENFORCE_DESKTOP_BINARY_SIZE: ${{ vars.FORMULA_ENFORCE_DESKTOP_BINARY_SIZE }}
          FORMULA_DESKTOP_BINARY_SIZE_LIMIT_MB: ${{ vars.FORMULA_DESKTOP_BINARY_SIZE_LIMIT_MB }}
        run: python3 scripts/desktop_binary_size_report.py --no-build --json-out desktop-binary-size-report.json

      - name: Upload desktop binary size report (JSON)
        if: always() && steps.desktop_bench.outputs.run == 'true' && steps.desktop_bench.outputs.kind == 'full'
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v4
        with:
          name: desktop-binary-size-report
          path: desktop-binary-size-report.json
          if-no-files-found: ignore

      - name: Fail on absolute target regression
        if: always() && steps.perf.outcome == 'failure'
        run: exit 1

  publish:
    name: Publish benchmark history (gh-pages)
    needs: benchmark
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    concurrency:
      # Avoid concurrent gh-pages pushes from multiple benchmark workflows.
      group: benchmark-gh-pages-publish
      cancel-in-progress: false
    permissions:
      actions: read
      contents: write
    # Publish whenever we have benchmark results for a push-to-main, even if the benchmark job failed,
    # so regressions (or flaky runs) still land in the gh-pages history for debugging.
    if: always() && github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v4

      - name: Download benchmark results artifact
        continue-on-error: true
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131 # v4
        with:
          name: perf-benchmark-results
          path: artifacts/perf-benchmark-results

      - name: Check benchmark results (skip publish when missing/empty)
        id: bench
        shell: bash
        run: |
          set -euo pipefail
          path="artifacts/perf-benchmark-results/benchmark-results.json"
          if [ ! -f "$path" ]; then
            echo "::notice::Missing ${path}; skipping gh-pages publish."
            echo "publish=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          if ! count="$(node -e 'const fs=require(\"fs\");const p=process.argv[1];const v=JSON.parse(fs.readFileSync(p,\"utf8\"));console.log(Array.isArray(v)?v.length:0);' "$path")"; then
            echo "::notice::Failed to parse ${path}; skipping gh-pages publish."
            echo "publish=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          if ! [[ "$count" =~ ^[0-9]+$ ]] || [ "$count" -lt 1 ]; then
            echo "::notice::Benchmark results file is empty; skipping gh-pages publish."
            echo "publish=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          echo "publish=true" >> "$GITHUB_OUTPUT"

      - name: Publish benchmark results
        if: steps.bench.outputs.publish == 'true'
        uses: benchmark-action/github-action-benchmark@4bdcce38c94cec68da58d012ac24b7b1155efe8b # v1
        with:
          tool: customSmallerIsBetter
          output-file-path: artifacts/perf-benchmark-results/benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '120%'
          comment-on-alert: true
          fail-on-alert: true
