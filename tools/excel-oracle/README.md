# Excel Oracle Compatibility Harness

This directory contains a **real Microsoft Excel** compatibility harness used to build an "oracle" dataset of expected formula results.

The intent is to continuously compare our formula engine against Excel across a growing, versioned corpus of cases.

## What this provides

- A deterministic, machine-readable **case corpus** (`tests/compatibility/excel-oracle/cases.json`)
- A Windows-only **Excel COM automation runner** that evaluates all cases in **real Excel** and exports results (`run-excel-oracle.ps1`)
- A **comparison tool** that diffs engine output vs Excel output and emits a mismatch report (`compare.py`)
- A lightweight **compatibility gate** that runs the engine + comparison on a bounded subset (`compat_gate.py`)
- A GitHub Actions workflow (`.github/workflows/excel-compat.yml`) wired to run on `windows-latest`

## Unified compatibility scorecard (corpus + Excel-oracle)

The Excel-oracle harness measures **calculation fidelity (L2)**. The compatibility corpus (`tools/corpus`) measures
**read (L1)** and **round-trip preservation (L4)**.

To merge both into a single markdown scorecard, run:

```bash
python tools/compat_scorecard.py --out-md compat_scorecard.md
```

In CI, the repo also includes a workflow-run aggregator (`.github/workflows/compat-scorecard.yml`) that downloads the
corpus + oracle artifacts and uploads a unified `compat-scorecard` report.

## Prerequisites (local generation)

To generate oracle data locally you must have:

- Windows
- Microsoft Excel installed (desktop)
- PowerShell (Windows PowerShell 5.1 or PowerShell 7+)
- Python 3 (for comparison/reporting, optional if you only generate data)

## CI note (Excel availability)

GitHub-hosted `windows-latest` runners typically **do not include Microsoft Excel**. To generate oracle data in CI you generally need a **self-hosted Windows runner** with Excel installed.

If you commit a pinned oracle dataset (see below), CI can still validate the engine even when Excel is not available.

See `tools/excel-oracle/self-hosted-runner.md` for notes on running Excel COM automation in CI.

## Case corpus

The canonical case list lives at:

`tests/compatibility/excel-oracle/cases.json`

It is generated by:

```powershell
python tools/excel-oracle/generate_cases.py --out tests/compatibility/excel-oracle/cases.json
```

The generator is deterministic; committing `cases.json` makes CI stable and reviewable.

The generator also validates the corpus against `shared/functionCatalog.json` to ensure:

- every `non_volatile` catalog function is exercised by at least one **case formula** (`case.formula`)
- `volatile` catalog functions are excluded (so pinned comparisons remain deterministic)

The Rust test suite enforces the same invariants (see
`crates/formula-engine/tests/excel_oracle_coverage.rs`) so drift is caught even if
`cases.json` is edited without re-running the generator.

## How to add oracle coverage for a new function

1) Add at least one new case to the appropriate module under:

`tools/excel-oracle/case_generators/`

Common buckets:

- `arith.py` (operators)
- `math.py`
- `engineering.py`
- `statistical.py` (aggregates + criteria semantics + stats/regression)
- `logical.py`
- `coercion.py` (type/boolean/blank coercion semantics)
- `text.py`
- `date_time.py`
- `lookup.py`
- `database.py`
- `financial.py`
- `spill.py` (dynamic arrays / spill behavior)
- `info.py`
- `lambda_cases.py` (LAMBDA / LET / MAP, etc.)
- `errors.py` (error creation/propagation)

Each module exposes `generate(cases, *, add_case, CellInput, ...) -> None` and is invoked
in a deterministic order from `tools/excel-oracle/generate_cases.py`.

Guidelines:

- Keep the corpus **deterministic**: do not add volatile functions (e.g. `RAND`, `NOW`).
- Keep the corpus **small**: the generator hard-caps at 2000 cases so it can run in real Excel in CI.
- Prefer locale-independent formulas/inputs where possible (avoid ambiguous date strings).

2) Regenerate the committed case corpus:

```bash
python tools/excel-oracle/generate_cases.py --out tests/compatibility/excel-oracle/cases.json
```

3) Validate:

```bash
python -m unittest tools/excel-oracle/tests/test_*.py
```

If you intentionally changed the case corpus, you may also need to re-pin the Excel dataset
(`tools/excel-oracle/pin_dataset.py`) so `tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json`
stays in sync.

## Generate oracle dataset from Excel

From repo root on Windows:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

Tip: pass `-DryRun` to see how many cases would be selected by the tag filters / `-MaxCases` without starting Excel.

To generate only a subset of cases (by tag):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath tests/compatibility/excel-oracle/datasets/excel-oracle.json `
  -IncludeTags SUM,IF,cmp `
  -ExcludeTags spill,dynarr
```

Note: `-IncludeTags` uses **OR semantics** (a case is included if it contains *any* of the included
tags). To require that a case contains *all* tags (AND semantics), use `-RequireTags`:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath tests/compatibility/excel-oracle/datasets/excel-oracle.json `
  -RequireTags odd_coupon,basis4
```

To generate only the **long odd-coupon** stub scenarios (`ODDF*` / `ODDL*`) for quick iteration / pinning,
use the small subset corpus:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_long_stub_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

To generate only the **odd-coupon negative yield / negative coupon** validation scenarios, use:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_validation_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

This subset corresponds to the cases tagged `odd_coupon_validation` in the canonical corpus.

To generate only the **odd-coupon boundary** date-equality scenarios (e.g. `issue == settlement`),
use:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_boundary_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

To generate only the **odd-coupon invalid schedule** scenarios (cases covering schedule alignment /
misalignment; some return `#NUM!`, while others are accepted by the engine today and should be
validated against real Excel), use:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_invalid_schedule_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

This subset corresponds to the cases tagged `odd_coupon` + `invalid_schedule` in the canonical corpus.

To generate only the **odd-coupon basis=4** scenarios (European 30/360), use:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_basis4_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

This subset corresponds to the cases tagged `odd_coupon` + `basis4` in the canonical corpus.

To regenerate the derived odd-coupon subset corpora (boundary + validation + long-stub + basis4 + invalid-schedule) from the
canonical corpus, run:

```bash
python tools/excel-oracle/regenerate_subset_corpora.py
```

To only verify the subset corpora are up to date (without rewriting files), run:

```bash
python tools/excel-oracle/regenerate_subset_corpora.py --check
```

To preview what would be written (case counts + paths) without writing files, run:

```bash
python tools/excel-oracle/regenerate_subset_corpora.py --dry-run
```

Note: this subset corpus reuses the **canonical case IDs** from `tests/compatibility/excel-oracle/cases.json`,
so you can map results back to the full corpus by `caseId` (useful when updating/pinning datasets).

The output JSON includes:

- Excel version/build metadata (because behavior can differ between Excel versions)
- A SHA-256 of the case corpus used
- Results encoded with a stable, typed representation (see below)

## Pin an oracle dataset (optional, for CI without Excel)

If you want CI to validate without running Excel, you can commit a pinned dataset file:

```bash
python tools/excel-oracle/pin_dataset.py \
  --dataset tests/compatibility/excel-oracle/datasets/excel-oracle.json \
  --pinned tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --versioned-dir tests/compatibility/excel-oracle/datasets/versioned
```

To validate the dataset and preview the output paths without writing files, use:

```bash
python tools/excel-oracle/pin_dataset.py --dataset /path/to/dataset.json --dry-run
```

Note: `pin_dataset.py` enforces that the dataset includes Excel version/build/OS metadata
from COM automation when pinning a real Excel dataset, to avoid accidentally pinning
something that simply sets `source.kind="excel"`.

`pin_dataset.py` also supports pinning a **synthetic CI baseline** from the in-repo Rust
engine (`crates/formula-excel-oracle`, where `source.kind == "formula-engine"`). In that
mode it re-tags the dataset as `source.kind="excel"` with `"unknown"` metadata and
embeds the original engine metadata under `source.syntheticSource`.

The workflow prefers `excel-oracle.pinned.json` if present.

To force Excel generation in the workflow when a pinned dataset exists, run the workflow manually (`workflow_dispatch`) and set `oracle_source=generate`.

## Regenerate the synthetic CI baseline (no Excel required)

When adding new deterministic functions (e.g. new STAT functions), you often need to update
multiple committed artifacts together to keep CI green:

- `shared/functionCatalog.json` (+ `shared/functionCatalog.mjs`)
- `tests/compatibility/excel-oracle/cases.json`
- `tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json`

To regenerate all of them from the current `formula-engine` implementation:

```bash
python tools/excel-oracle/regenerate_synthetic_baseline.py
```

To preview the commands it would run (without executing them or writing files), use:

```bash
python tools/excel-oracle/regenerate_synthetic_baseline.py --dry-run
```

### Incremental pinned dataset updates (merge-friendly)

When you *only add new cases* to `cases.json` (i.e. existing case IDs remain valid), regenerating the
entire pinned dataset can create very large diffs and frequent merge conflicts.

You can instead update the pinned dataset incrementally to fill in only the missing case results:

```bash
python tools/excel-oracle/update_pinned_dataset.py
```

To preview what would happen (missing cases, whether the engine would run, etc.) without writing
files or running Cargo, use:

```bash
python tools/excel-oracle/update_pinned_dataset.py --dry-run
```

By default this also refreshes the matching **versioned** dataset copy under:

* `tests/compatibility/excel-oracle/datasets/versioned/`

so `tools/excel-oracle/compat_gate.py` (which prefers the versioned dataset when present) stays in
sync. To update only the pinned file, pass `--no-versioned`.

If you generated **real Excel** results for a subset of cases and want to overwrite the synthetic
baseline values in the pinned dataset (while keeping the rest of the corpus unchanged), pass
`--merge-results` and `--overwrite-existing`:

```bash
python tools/excel-oracle/update_pinned_dataset.py \
  --merge-results /path/to/excel-results.json \
  --overwrite-existing \
  --no-engine
```

For a one-command flow that runs Excel on a subset corpus and patches the pinned dataset, see:

* `tools/excel-oracle/patch-pinned-dataset-with-excel.ps1`

Tip: pass `-DryRun` to preview the commands without running Excel or modifying the pinned dataset.

Example (patch only the odd-coupon negative yield / negative coupon validation scenarios):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_validation_cases.json
```

Example (patch only the odd-coupon boundary date-equality scenarios):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_boundary_cases.json
```

Example (patch only the odd-coupon basis=4 (European 30/360) scenarios):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_basis4_cases.json
```

Example (patch only the odd-coupon long-stub scenarios):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_long_stub_cases.json
```

Example (patch only the odd-coupon schedule alignment / misalignment scenarios (invalid schedule cases)):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_invalid_schedule_cases.json
```

You can also patch by **tag filter** without a dedicated subset file by running against the
canonical corpus and passing through `-IncludeTags` (OR semantics), `-RequireTags` (AND semantics),
and/or `-ExcludeTags`:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tests/compatibility/excel-oracle/cases.json `
  -IncludeTags odd_coupon_validation
```

Example (patch only the odd-coupon `basis=4` cases using AND tag filtering):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tests/compatibility/excel-oracle/cases.json `
  -RequireTags odd_coupon,basis4
```

This script patches the pinned dataset by invoking `update_pinned_dataset.py`, which preserves
existing results and updates the `caseSet.sha256`/`caseSet.count` metadata. The patch flow uses
`--no-engine`, so only the provided Excel results are merged/overwritten (no engine fallback).

When you patch a synthetic baseline with **real Excel** results, `update_pinned_dataset.py` records
a small provenance entry under `source.patches` in the pinned dataset so itâ€™s clear which case IDs
were overwritten (and which Excel version/build produced them).

Important: by default the updater refuses to fill missing cases if the pinned dataset appears to be
generated by real Excel (no `source.syntheticSource` metadata), because that would mix engine results
into an Excel oracle dataset. In that scenario, generate new Excel results and merge them with
`--merge-results` (or use `--force-engine` if you explicitly want a synthetic baseline).

This will:

1. Regenerate the function catalog from `formula-engine`'s inventory registry.
2. Regenerate the oracle case corpus (and validate coverage).
3. Evaluate the full corpus using `crates/formula-excel-oracle`.
4. Pin the results as a synthetic dataset for CI.

## Compare formula-engine output vs Excel oracle

### One-command gate (CI-friendly)

From repo root:

```bash
python tools/excel-oracle/compat_gate.py
```

The gate supports tier presets:

```bash
python tools/excel-oracle/compat_gate.py --tier smoke   # default, CI-friendly slice
python tools/excel-oracle/compat_gate.py --tier p0      # broader common-function slice
python tools/excel-oracle/compat_gate.py --tier full    # full corpus (no include-tag filtering)
```

To inspect which datasets/tags will be selected (and to see the exact engine + compare commands)
without running `cargo`, use:

```bash
python tools/excel-oracle/compat_gate.py --dry-run --tier smoke --max-cases 10
```

See `tests/compatibility/excel-oracle/README.md` for the exact tag presets and
recommended runtime tradeoffs.

This runs the in-repo engine adapter (`crates/formula-excel-oracle`) against a curated tag set,
compares against the pinned dataset in `tests/compatibility/excel-oracle/datasets/versioned/`,
writes reports under `tests/compatibility/excel-oracle/reports/`, and exits non-zero on mismatch.

Note on **synthetic baselines**: the default pinned dataset in this repo may be a *synthetic CI baseline*
(`source.syntheticSource`), meaning it is **not** generated by real Microsoft Excel. `compat_gate.py`
prints an explicit warning when it detects a synthetic expected dataset. To enforce that CI is using a
real Excel dataset (for example on a self-hosted Windows runner that can run `run-excel-oracle.ps1`),
pass:

```bash
python tools/excel-oracle/compat_gate.py --require-real-excel
```

### Manual flow

1) Produce engine results JSON (same schema as Excel output). The intended flow is that your engine exposes a CLI that can evaluate the case corpus and emit results.

This repo includes a reference implementation for the Rust `formula-engine` at:

`crates/formula-excel-oracle/` (run with `cargo run -p formula-excel-oracle -- --cases ... --out ...`).

It also supports `--include-tag` / `--exclude-tag` for evaluating a filtered subset of the corpus.

2) Compare:

```bash
python tools/excel-oracle/compare.py \
  --cases tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.json \
  --actual tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report tests/compatibility/excel-oracle/reports/mismatch-report.json
```

To preview how many cases would be compared (after tag filtering / `--max-cases`) without writing a report file, use:

```bash
python tools/excel-oracle/compare.py --dry-run \
  --cases tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report tests/compatibility/excel-oracle/reports/mismatch-report.json
```

The report includes `caseId`, `formula`, `inputs`, `expected`, `actual`, and a reason.

`compare.py` also verifies that the `caseSet.sha256` embedded in the datasets matches the current `cases.json`, to prevent stale-oracle comparisons.

### Tag filtering

Cases include `tags` (e.g. `["logical","IF"]`). You can restrict comparisons to a subset:

```bash
python tools/excel-oracle/compare.py \
  --cases tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report tests/compatibility/excel-oracle/reports/mismatch-report.json \
  --include-tag IF --include-tag SUM --include-tag cmp
```

### Numeric tolerances (iterative functions)

`compare.py` defaults to tight numeric tolerances (`abs=rel=1e-9`). Some functions are inherently
iterative (for example yield solvers), and can differ from Excel by small floating point amounts even
when the math is correct.

You can override numeric tolerances for tagged subsets without loosening the entire corpus:

```bash
python tools/excel-oracle/compare.py \
  --cases    tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual   tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report   tests/compatibility/excel-oracle/reports/mismatch-report.json \
  --tag-abs-tol odd_coupon=1e-6 \
  --tag-rel-tol odd_coupon=1e-6
```

Note: `tools/excel-oracle/compat_gate.py` already applies `odd_coupon=1e-6` by default.

## Value encoding

Excel values are encoded to avoid ambiguity between:

- blank vs 0
- numbers vs error codes
- scalar vs spilled array results

See `tools/excel-oracle/value-encoding.md`.

## Schemas

JSON Schemas (for editor validation) live in `tools/excel-oracle/schemas/`.
