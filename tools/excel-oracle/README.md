# Excel Oracle Compatibility Harness

This directory contains a **real Microsoft Excel** compatibility harness used to build an "oracle" dataset of expected formula results.

The intent is to continuously compare our formula engine against Excel across a growing, versioned corpus of cases.

## What this provides

- A deterministic, machine-readable **case corpus** (`tests/compatibility/excel-oracle/cases.json`)
- A Windows-only **Excel COM automation runner** that evaluates all cases in **real Excel** and exports results (`run-excel-oracle.ps1`)
- A **comparison tool** that diffs engine output vs Excel output and emits a mismatch report (`compare.py`)
- A lightweight **compatibility gate** that runs the engine + comparison on a bounded subset (`compat_gate.py`)
- A GitHub Actions workflow (`.github/workflows/excel-compat.yml`) wired to run on `windows-latest`

## Prerequisites (local generation)

To generate oracle data locally you must have:

- Windows
- Microsoft Excel installed (desktop)
- PowerShell (Windows PowerShell 5.1 or PowerShell 7+)
- Python 3 (for comparison/reporting, optional if you only generate data)

## CI note (Excel availability)

GitHub-hosted `windows-latest` runners typically **do not include Microsoft Excel**. To generate oracle data in CI you generally need a **self-hosted Windows runner** with Excel installed.

If you commit a pinned oracle dataset (see below), CI can still validate the engine even when Excel is not available.

See `tools/excel-oracle/self-hosted-runner.md` for notes on running Excel COM automation in CI.

## Case corpus

The canonical case list lives at:

`tests/compatibility/excel-oracle/cases.json`

It is generated by:

```powershell
python tools/excel-oracle/generate_cases.py --out tests/compatibility/excel-oracle/cases.json
```

The generator is deterministic; committing `cases.json` makes CI stable and reviewable.

The generator also validates the corpus against `shared/functionCatalog.json` to ensure:

- every `non_volatile` catalog function is exercised by at least one **case formula** (`case.formula`)
- `volatile` catalog functions are excluded (so pinned comparisons remain deterministic)

The Rust test suite enforces the same invariants (see
`crates/formula-engine/tests/excel_oracle_coverage.rs`) so drift is caught even if
`cases.json` is edited without re-running the generator.

## How to add oracle coverage for a new function

1) Add at least one new case to the appropriate module under:

`tools/excel-oracle/case_generators/`

Common buckets:

- `arith.py` (operators)
- `math.py`
- `engineering.py`
- `statistical.py` (aggregates + criteria semantics + stats/regression)
- `logical.py`
- `coercion.py` (type/boolean/blank coercion semantics)
- `text.py`
- `date_time.py`
- `lookup.py`
- `database.py`
- `financial.py`
- `spill.py` (dynamic arrays / spill behavior)
- `info.py`
- `lambda_cases.py` (LAMBDA / LET / MAP, etc.)
- `errors.py` (error creation/propagation)

Each module exposes `generate(cases, *, add_case, CellInput, ...) -> None` and is invoked
in a deterministic order from `tools/excel-oracle/generate_cases.py`.

Guidelines:

- Keep the corpus **deterministic**: do not add volatile functions (e.g. `RAND`, `NOW`).
- Keep the corpus **small**: the generator hard-caps at 2000 cases so it can run in real Excel in CI.
- Prefer locale-independent formulas/inputs where possible (avoid ambiguous date strings).

2) Regenerate the committed case corpus:

```bash
python tools/excel-oracle/generate_cases.py --out tests/compatibility/excel-oracle/cases.json
```

3) Validate:

```bash
python -m unittest tools/excel-oracle/tests/test_*.py
```

If you intentionally changed the case corpus, you may also need to re-pin the Excel dataset
(`tools/excel-oracle/pin_dataset.py`) so `tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json`
stays in sync.

## Generate oracle dataset from Excel

From repo root on Windows:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

To generate only a subset of cases (by tag):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tests/compatibility/excel-oracle/cases.json `
  -OutPath tests/compatibility/excel-oracle/datasets/excel-oracle.json `
  -IncludeTags SUM,IF,cmp `
  -ExcludeTags spill,dynarr
```

To generate only the **long odd-coupon** stub scenarios (`ODDF*` / `ODDL*`) for quick iteration / pinning,
use the small subset corpus:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_long_stub_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

To generate only the **odd-coupon negative yield / negative coupon** validation scenarios, use:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_validation_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

This subset corresponds to the cases tagged `odd_coupon_validation` in the canonical corpus.

To generate only the **odd-coupon boundary** date-equality scenarios (e.g. `issue == settlement`),
use:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/run-excel-oracle.ps1 `
  -CasesPath tools/excel-oracle/odd_coupon_boundary_cases.json `
  -OutPath  tests/compatibility/excel-oracle/datasets/excel-oracle.json
```

To regenerate the derived odd-coupon subset corpora (boundary + validation + long-stub) from the
canonical corpus, run:

```bash
python tools/excel-oracle/regenerate_subset_corpora.py
```

Note: this subset corpus reuses the **canonical case IDs** from `tests/compatibility/excel-oracle/cases.json`,
so you can map results back to the full corpus by `caseId` (useful when updating/pinning datasets).

The output JSON includes:

- Excel version/build metadata (because behavior can differ between Excel versions)
- A SHA-256 of the case corpus used
- Results encoded with a stable, typed representation (see below)

## Pin an oracle dataset (optional, for CI without Excel)

If you want CI to validate without running Excel, you can commit a pinned dataset file:

```bash
python tools/excel-oracle/pin_dataset.py \
  --dataset tests/compatibility/excel-oracle/datasets/excel-oracle.json \
  --pinned tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --versioned-dir tests/compatibility/excel-oracle/datasets/versioned
```

Note: `pin_dataset.py` enforces that the dataset includes Excel version/build/OS metadata
from COM automation when pinning a real Excel dataset, to avoid accidentally pinning
something that simply sets `source.kind="excel"`.

`pin_dataset.py` also supports pinning a **synthetic CI baseline** from the in-repo Rust
engine (`crates/formula-excel-oracle`, where `source.kind == "formula-engine"`). In that
mode it re-tags the dataset as `source.kind="excel"` with `"unknown"` metadata and
embeds the original engine metadata under `source.syntheticSource`.

The workflow prefers `excel-oracle.pinned.json` if present.

To force Excel generation in the workflow when a pinned dataset exists, run the workflow manually (`workflow_dispatch`) and set `oracle_source=generate`.

## Regenerate the synthetic CI baseline (no Excel required)

When adding new deterministic functions (e.g. new STAT functions), you often need to update
multiple committed artifacts together to keep CI green:

- `shared/functionCatalog.json` (+ `shared/functionCatalog.mjs`)
- `tests/compatibility/excel-oracle/cases.json`
- `tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json`

To regenerate all of them from the current `formula-engine` implementation:

```bash
python tools/excel-oracle/regenerate_synthetic_baseline.py
```

### Incremental pinned dataset updates (merge-friendly)

When you *only add new cases* to `cases.json` (i.e. existing case IDs remain valid), regenerating the
entire pinned dataset can create very large diffs and frequent merge conflicts.

You can instead update the pinned dataset incrementally to fill in only the missing case results:

```bash
python tools/excel-oracle/update_pinned_dataset.py
```

If you generated **real Excel** results for a subset of cases and want to overwrite the synthetic
baseline values in the pinned dataset (while keeping the rest of the corpus unchanged), pass
`--merge-results` and `--overwrite-existing`:

```bash
python tools/excel-oracle/update_pinned_dataset.py \
  --merge-results /path/to/excel-results.json \
  --overwrite-existing \
  --no-engine
```

For a one-command flow that runs Excel on a subset corpus and patches the pinned dataset, see:

* `tools/excel-oracle/patch-pinned-dataset-with-excel.ps1`

Example (patch only the odd-coupon negative yield / negative coupon validation scenarios):

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tools/excel-oracle/odd_coupon_validation_cases.json
```

You can also patch by **tag filter** without a dedicated subset file by running against the
canonical corpus and passing through `-IncludeTags`/`-ExcludeTags`:

```powershell
powershell -ExecutionPolicy Bypass -File tools/excel-oracle/patch-pinned-dataset-with-excel.ps1 `
  -SubsetCasesPath tests/compatibility/excel-oracle/cases.json `
  -IncludeTags odd_coupon_validation
```

This script preserves existing results, updates the `caseSet.sha256`/`caseSet.count` metadata, and
only evaluates missing cases via `crates/formula-excel-oracle`.

Important: by default the updater refuses to fill missing cases if the pinned dataset appears to be
generated by real Excel (no `source.syntheticSource` metadata), because that would mix engine results
into an Excel oracle dataset. In that scenario, generate new Excel results and merge them with
`--merge-results` (or use `--force-engine` if you explicitly want a synthetic baseline).

This will:

1. Regenerate the function catalog from `formula-engine`'s inventory registry.
2. Regenerate the oracle case corpus (and validate coverage).
3. Evaluate the full corpus using `crates/formula-excel-oracle`.
4. Pin the results as a synthetic dataset for CI.

## Compare formula-engine output vs Excel oracle

### One-command gate (CI-friendly)

From repo root:

```bash
python tools/excel-oracle/compat_gate.py
```

The gate supports tier presets:

```bash
python tools/excel-oracle/compat_gate.py --tier smoke   # default, CI-friendly slice
python tools/excel-oracle/compat_gate.py --tier p0      # broader common-function slice
python tools/excel-oracle/compat_gate.py --tier full    # full corpus (no include-tag filtering)
```

See `tests/compatibility/excel-oracle/README.md` for the exact tag presets and
recommended runtime tradeoffs.

This runs the in-repo engine adapter (`crates/formula-excel-oracle`) against a curated tag set,
compares against the pinned dataset in `tests/compatibility/excel-oracle/datasets/versioned/`,
writes reports under `tests/compatibility/excel-oracle/reports/`, and exits non-zero on mismatch.

### Manual flow

1) Produce engine results JSON (same schema as Excel output). The intended flow is that your engine exposes a CLI that can evaluate the case corpus and emit results.

This repo includes a reference implementation for the Rust `formula-engine` at:

`crates/formula-excel-oracle/` (run with `cargo run -p formula-excel-oracle -- --cases ... --out ...`).

It also supports `--include-tag` / `--exclude-tag` for evaluating a filtered subset of the corpus.

2) Compare:

```bash
python tools/excel-oracle/compare.py \
  --cases tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.json \
  --actual tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report tests/compatibility/excel-oracle/reports/mismatch-report.json
```

The report includes `caseId`, `formula`, `inputs`, `expected`, `actual`, and a reason.

`compare.py` also verifies that the `caseSet.sha256` embedded in the datasets matches the current `cases.json`, to prevent stale-oracle comparisons.

### Tag filtering

Cases include `tags` (e.g. `["logical","IF"]`). You can restrict comparisons to a subset:

```bash
python tools/excel-oracle/compare.py \
  --cases tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report tests/compatibility/excel-oracle/reports/mismatch-report.json \
  --include-tag IF --include-tag SUM --include-tag cmp
```

### Numeric tolerances (iterative functions)

`compare.py` defaults to tight numeric tolerances (`abs=rel=1e-9`). Some functions are inherently
iterative (for example yield solvers), and can differ from Excel by small floating point amounts even
when the math is correct.

You can override numeric tolerances for tagged subsets without loosening the entire corpus:

```bash
python tools/excel-oracle/compare.py \
  --cases    tests/compatibility/excel-oracle/cases.json \
  --expected tests/compatibility/excel-oracle/datasets/excel-oracle.pinned.json \
  --actual   tests/compatibility/excel-oracle/datasets/engine-results.json \
  --report   tests/compatibility/excel-oracle/reports/mismatch-report.json \
  --tag-abs-tol odd_coupon=1e-6 \
  --tag-rel-tol odd_coupon=1e-6
```

Note: `tools/excel-oracle/compat_gate.py` already applies `odd_coupon=1e-6` by default.

## Value encoding

Excel values are encoded to avoid ambiguity between:

- blank vs 0
- numbers vs error codes
- scalar vs spilled array results

See `tools/excel-oracle/value-encoding.md`.

## Schemas

JSON Schemas (for editor validation) live in `tools/excel-oracle/schemas/`.
